{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangChain: Basic concepts recap**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Preprocessing the data\n",
    "\n",
    "LangChain provides tools that help structure documents for convenient use with LLMs. Document loaders simplify the process of loading data into documents and text splitters break down length pieces of text into smaller chunks for better processing. Finally, indexing involvides creating a structured database of information that the langiage model can query to enhance its understanding and responses.\n",
    "\n",
    "1.1 Document Loaders\n",
    "* loading documents into structured data, into `Document` objects\n",
    "* over 100 loaders and integrations with AirByte and Unstructured, etc.\n",
    "* CSVLoader - each row into a separate `Document`\n",
    "* TextLoader - text files\n",
    "* DirectoryReader - files in a directory\n",
    "* UnstructuredMarkdownLoader - markdown files\n",
    "* PyPDFLoader - loading PDF files\n",
    "* WikipediaLoader - content of specified Wikipedia page\n",
    "* UnstructuredURLLoader - reading from public web pages\n",
    "* Proprietary Loaders - additional authentication/setup required, e.g. GoogleDriveLoader, MongodbLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader, WikipediaLoader\n",
    "\n",
    "# CSV file\n",
    "loader = CSVLoader('./data/data.csv')\n",
    "documents = loader.load()\n",
    "\n",
    "# access content and metadata\n",
    "for document in documents:\n",
    "    content = document.page_content\n",
    "    metadata = document.metadata\n",
    "\n",
    "# Wikipedia page\n",
    "loader = WikipediaLoader('Machine_learning')\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Document transformers (chunking methods)\n",
    "* splitting documents into smaller segments\n",
    "* necessary owing to context window limitations, e.g. GPT-4 (8000), ada-002 (8000)\n",
    "* Transformation strategies :\n",
    "\n",
    "1.2.1 Fixed-size chunks - fixed size sufficient for semantically meaningful paragraphs and some overlap, ensuring continuity and context preservation, improving coherence and accuracy, e.g. `CharacterTextSplitter` splits  every N characters/tokens if configured\n",
    "\n",
    "1.2.2. Variable-sized chunks - partition based on content characteristics, such as EoS punctuation marks, EoL markers or features in NLP libraries, ensures preservation of coherent and contextually intact content, e.g. `RecursiveCharacterTextSplitter`\n",
    "\n",
    "1.2.3 Customized chunking - you may want to append document title to middle chunks to prevent context loss, e.g. `MarkdownHeaderTextSplitter`\n",
    "\n",
    "* drawback is the risk of losing vital context related to the overall document, each chunk may only partially capture the nuances and interconnected elements present in the full text, leading to fragmented understanding\n",
    "\n",
    "1.3 Indexing\n",
    "* storing and organizing data into a vector store, essential for efficient retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Models\n",
    "\n",
    "2.1 LLMs\n",
    "* `LLM` class for interfacing with various model providers such as OpenAI, Cohere and HuggingFace.\n",
    "* Chatmodels are a variation of language models that use a message-based i/p and o/p system, has three types of messages -\n",
    "\n",
    "2.1.1 `SystemMessage` - sets the behaviour and objectives of the chat model\n",
    "\n",
    "2.1.2 - `HumanMessage` - input prompts\n",
    "\n",
    "2.1.3 `AIMessage` - responses from the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'key'\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content='You are a helpful assistant.'\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content='What is the capital of France?'\n",
    "    )\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Embedding models\n",
    "* standardized interface for various embedding model providers like OpenAI, Cohere and HuggingFace\n",
    "* transform text into vector representations, enabling semantic search through text similarity in vector space\n",
    "* consistent o/p dimensionality, regardless of input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "embeddings = embedding_model.embed_documents(\n",
    "    [\n",
    "        'Hi there!',\n",
    "        'Oh, hello!'\n",
    "    ]\n",
    ")\n",
    "print('Number of documents embedded:', len(embeddings))\n",
    "print('Dimension of each embedding:', len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Vector Stores\n",
    "* embeddings are high-dimensional vectors that capture the sementics of textual data\n",
    "* traditional databases are not optimized for high-dimensional data, vector stores, on the other hand can effectively store and search these embeddings\n",
    "* advantages of using vector stores in LangChain - speed, scalability and precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Retrievers\n",
    "* interfaces that return documents in response to a query - most basic being similarity metrics like cosine similarity\n",
    "* LangChain offers more advanced retrieval strategies -\n",
    "\n",
    "3.1 Parent document retriever - creates multiple embeddings, allowing to look up smaller chunks but return larger context, context from parent document is used for generating final answer\n",
    "\n",
    "3.2 Self-query retriever - generate several filters based on the user prompt and apply them to the document metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Chains\n",
    "* powerful, reusable components that can be linked together to perform complex tasks\n",
    "* integrating prompt templates with LLMS using chains allows a powerful synergy, taking output of one LLM and using it as input for another makes it feasible to connect multiple prompts sequentially\n",
    "* allows integrating LLMs with other components such as long-term memory and output guarding\n",
    "* chains can enhance overall depth and quality of interactions\n",
    "* two classes - `LLMChain` and `SequentialChain`\n",
    "\n",
    "4.1 `LLMChain`\n",
    "* simplest form of chain that transforms user inputs using a `PromptTemplate`\n",
    "* the prompt object defines the parameters of our request to the model and determines the expected format of the output\n",
    "* then we can use `LLMChain` to tie the prompt and model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "template = '''List all the colours in a rainbow.'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[],\n",
    "    output_parser=StrOutputParser()\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=chat\n",
    ")\n",
    "\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LCEL\n",
    "prompt = PromptTemplate.from_template(\n",
    "    'List all the colours in a {item}.'\n",
    ")\n",
    "\n",
    "runnable = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    "runnable.invoke({'item': 'rainbow'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 `Sequential`\n",
    "* making subsequent calls to an LLM\n",
    "* especially beneficial for using the outputs of one call as the input for the next, streamlining the process and enabling complex interactions across various applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "post_prompt = PromptTemplate.from_template(\n",
    "    '''You are a business owner. Given the theme of a post, write a social media article.'\n",
    "\n",
    "    Theme: {theme}\n",
    "    Content: This is social mesia post based on the theme above.'''\n",
    ")\n",
    "\n",
    "review_prompt = PromptTemplate.from_template(\n",
    "    '''You are an expert social media manager. Given the post, your job is to review it.\n",
    "     \n",
    "Social media post: {post}\n",
    "Review from a Social Media Expert:'''\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {'post': post_prompt | llm | StrOutputParser()}\n",
    "    | review_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\n",
    "    {'theme': 'Having a black friday sale with 50% off on everything.'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing both the post and the review\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "post_chain = post_prompt | llm | StrOutputParser()\n",
    "review_chain = review_prompt | llm | StrOutputParser()\n",
    "chain = {'post': post_chain} | RunnablePassthrough.assign(review=review_chain)\n",
    "chain.invoke(\n",
    "    {\n",
    "        'theme': 'Having a black friday sale with 50% off on everything.'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Memory\n",
    "* backbone for maintaining context in ongoing dialogues\n",
    "* LangChain's Memory module addresses the issues that traditional conversational models face with maintaining context, by storing both input and output messages in a structured manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LlamaIndex: Precision and Simplicity in Information Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Stores\n",
    "\n",
    "* storage of large, high-dimensional data and semantic search tools\n",
    "* transcends traditional keyword matching be seeking information that aligns conceptually with the user query\n",
    "\n",
    "Data Connectors\n",
    "* `Readers` in LlamaIndex parse and convert data into a simplified `Document` representation consisting of text and basic metadata\n",
    "\n",
    "Nodes\n",
    "* once data is ingested as documents, it passes through a processing structure that transforms these documents into `Node` objects\n",
    "* Nodes are smaller, more granular data units created from the original documents\n",
    "* besides content, nodes also contain metadata and contextual information\n",
    "* `NodeParser` class designed to convert the content of the documents into structured nodes automatically, `SimpleNodeParser` converts a list of document objects into nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=20)\n",
    "nodes = parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indices\n",
    "\n",
    "* initial step for storing information in a database, essentially transforms the unstructured data into embeddings that capture semantic meaning and optimize the data format so that it can be easily accessed and queried\n",
    "* variety of index types -\n",
    "\n",
    "1. Summary Index - extracts a summary from each document and stores it with all the nodes in that document\n",
    "2. Vector Store Index - generates embeddings during index construction to identify the top-k most similar nodes in response to a query, suitable for small-scale applications and easily scalable to accomodate larger datasets using high-performance vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Engines\n",
    "\n",
    "* wrapper around a retriever and a response synthesizer\n",
    "* uses the query string to fetch nodes and sends them to LLM to generate a response\n",
    "* indexes can also function solely as retrievers for fetching documents relevant to the query\n",
    "* enables creating of Custom Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routers\n",
    "* determine the most appropriate retriever for extracting context from the knowledge base\n",
    "* selects the optimal query engine for each task, improving performance and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and Loading indexes locally\n",
    "* saving the index data, which includes nodes and their associated embeddings to disk\n",
    "* done using the `persist()` method from the `storage_context` object related to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist()\n",
    "# This saves the data in the 'storage' by default\n",
    "\n",
    "# Let's see if our index already exists in storage.\n",
    "if not os.path.exists(\"./storage\"):\n",
    "    # If not, we'll load the Wikipedia data and create a new index\n",
    "\tWikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\tloader = WikipediaReader()\n",
    "    documents = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence'])\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # Index storing\n",
    "    index.storage_context.persist()\n",
    "else:\n",
    "    # If the index already exists, we'll just load it:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Langchain vs. LlamaIndex**\n",
    "\n",
    "LlamaIndex: processing, structuring and accessing private or domain-specific data with a focus on specific LLM interactions, tasks with high precision and quality, main strength is linking LLMs to any data source\n",
    "\n",
    "Langchain: dynamic, suited for context-rich interactions, effective for chatbots and virtual assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Requirements\n",
    "1. Python - Version 3.7 or newer is required.\n",
    "2. An active account on OpenAI, along with an OpenAI API key.\n",
    "3. An Activeloop Deep Lake account, complete with a Deep Lake API key.\n",
    "4. A 'classic' personal token from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python virtual environment\n",
    "!python3 -m venv repo-ai\n",
    "!source repo-ai/bin/activate\n",
    "%pip install llama-index deeplake openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does LLamaIndex work?**\n",
    "\n",
    "In the context of leveraging LlamaIndex for data-driven applications, the underlying logic and workflow are pretty simple. Here's a breakdown:\n",
    "\n",
    "Load Documents: The first step involves loading your raw data into the system. You can do this manually, directly inputting the data, or through a data loader that automates the process. LlamaIndex offers specialized data loaders that can ingest data from various sources, transforming them into Document objects, and you can find many plugins on Llama Hub. This is a crucial step as it sets the stage for the subsequent data manipulation and querying functionalities.\n",
    "Parse the Documents into Nodes: Once the documents are loaded, they are parsed into Nodes, essentially structured data units. These Nodes contain chunks of the original documents and carry valuable metadata and relationship information. This parsing process is vital as it organizes the raw data into a structured format, making it easier and more efficient for the system to handle.\n",
    "\n",
    "Construct an Index from Nodes or Documents: After the Nodes are prepared, an index is constructed to make the data searchable and queryable. Depending on your needs, this index can be built directly from the original documents or the parsed Nodes. The index is often stored in structures like VectorStoreIndex, optimized for quick data retrieval. This step is the system's heart, turning your structured data into a robust, queryable database.\n",
    "\n",
    "Query the Index: With the index in place, the final step is to query it. A query engine is initialized, allowing you to make natural language queries against the indexed data. This is where the magic happens: you can conversationally ask the system questions, and it will sift through the indexed data to provide accurate and relevant answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN=\"YOUR_GH_CLASSIC_TOKEN\"\n",
    "OPENAI_API_KEY=\"YOUR_OPENAI_KEY\"\n",
    "ACTIVELOOP_TOKEN=\"YOUR_ACTIVELOOP_TOKEN\"\n",
    "DATASET_PATH=\"hub://YOUR_ORG/repository_vector_store\"Copy\n",
    "\n",
    "# main.py\n",
    "import os\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "from llama_index import download_loader\n",
    "from llama_hub.github_repo import GithubRepositoryReader, GithubClient\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch and set API keys\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "dataset_path = os.getenv(\"DATASET_PATH\")\n",
    "\n",
    "\n",
    "def parse_github_url(url):\n",
    "    pattern = r\"https://github\\.com/([^/]+)/([^/]+)\"\n",
    "    match = re.match(pattern, url)\n",
    "    return match.groups() if match else (None, None)\n",
    "\n",
    "\n",
    "def validate_owner_repo(owner, repo):\n",
    "    return bool(owner) and bool(repo)\n",
    "\n",
    "\n",
    "def initialize_github_client():\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    return GithubClient(github_token)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Check for OpenAI API key\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not openai_api_key:\n",
    "        raise EnvironmentError(\"OpenAI API key not found in environment variables\")\n",
    "\n",
    "    # Check for GitHub Token\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not github_token:\n",
    "        raise EnvironmentError(\"GitHub token not found in environment variables\")\n",
    "\n",
    "    # Check for Activeloop Token\n",
    "    active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "    if not active_loop_token:\n",
    "        raise EnvironmentError(\"Activeloop token not found in environment variables\")\n",
    "\n",
    "    github_client = initialize_github_client()\n",
    "    download_loader(\"GithubRepositoryReader\")\n",
    "\n",
    "    github_url = input(\"Please enter the GitHub repository URL: \")\n",
    "    owner, repo = parse_github_url(github_url)\n",
    "\n",
    "    while True:\n",
    "        owner, repo = parse_github_url(github_url)\n",
    "        if validate_owner_repo(owner, repo):\n",
    "            loader = GithubRepositoryReader(\n",
    "                github_client,\n",
    "                owner=owner,\n",
    "                repo=repo,\n",
    "                filter_file_extensions=(\n",
    "                    [\".py\", \".js\", \".ts\", \".md\"],\n",
    "                    GithubRepositoryReader.FilterType.INCLUDE,\n",
    "                ),\n",
    "                verbose=False,\n",
    "                concurrent_requests=5,\n",
    "            )\n",
    "            print(f\"Loading {repo} repository by {owner}\")\n",
    "            docs = loader.load_data(branch=\"main\")\n",
    "            print(\"Documents uploaded:\")\n",
    "            for doc in docs:\n",
    "                print(doc.metadata)\n",
    "            break  # Exit the loop once the valid URL is processed\n",
    "        else:\n",
    "            print(\"Invalid GitHub URL. Please try again.\")\n",
    "            github_url = input(\"Please enter the GitHub repository URL: \")\n",
    "\n",
    "    print(\"Uploading to vector store...\")\n",
    "\n",
    "    # ====== Create vector store and upload data ======\n",
    "\n",
    "    vector_store = DeepLakeVectorStore(\n",
    "        dataset_path=dataset_path,\n",
    "        overwrite=True,\n",
    "        runtime={\"tensor_db\": True},\n",
    "    )\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    # Include a simple question to test.\n",
    "    intro_question = \"What is the repository about?\"\n",
    "    print(f\"Test question: {intro_question}\")\n",
    "    print(\"=\" * 50)\n",
    "    answer = query_engine.query(intro_question)\n",
    "\n",
    "    print(f\"Answer: {textwrap.fill(str(answer), 100)} \\n\")\n",
    "    while True:\n",
    "        user_question = input(\"Please enter your question (or type 'exit' to quit): \")\n",
    "        if user_question.lower() == \"exit\":\n",
    "            print(\"Exiting, thanks for chatting!\")\n",
    "            break\n",
    "\n",
    "        print(f\"Your question: {user_question}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        answer = query_engine.query(user_question)\n",
    "        print(f\"Answer: {textwrap.fill(str(answer), 100)} \\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialization and Environment Setup**\n",
    "\n",
    "Import Required Libraries: The script starts by importing all the necessary modules and packages.\n",
    "\n",
    "Load Environment Variables: Using dotenv, it loads environment variables stored in the .env file. This is where API keys and tokens are stored securely.\n",
    "\n",
    "Helper Functions\n",
    "1. parse_github_url(url): This function takes a GitHub URL and extracts the repository owner and name using regular expressions.\n",
    "2. validate_owner_repo(owner, repo): Validates that both the repository owner and name are present.\n",
    "3. initialize_github_client(): Initializes the GitHub client using the token fetched from the environment variables.\n",
    "\n",
    "API Key Checks: Before proceeding, the script checks for the presence of the OpenAI API key, GitHub token, and Activeloop token, raising an error if any are missing.\n",
    "\n",
    "Initialize GitHub Client: Calls initialize_github_client() to get a GitHub client instance.\n",
    "\n",
    "User Input for GitHub URL: Asks the user to input a GitHub repository URL.\n",
    "\n",
    "URL Parsing and Validation: Parses the URL to get the repository owner and name and validates them.\n",
    "\n",
    "Data Loading: If the URL is valid, it uses GithubRepositoryReader from llama_index to load the repository data, specifically Python and Markdown files.\n",
    "\n",
    "Indexing: The loaded data is then indexed using VectorStoreIndex and stored in a DeepLake vector store. This makes the data queryable.\n",
    "\n",
    "Query Engine Initialization: Initializes a query engine based on the indexed data.\n",
    "\n",
    "Test Query: Performs a test query to demonstrate the system's operation.\n",
    "\n",
    "User Queries: Enters a loop where the user can input natural language queries to interact with the indexed GitHub repository. The loop continues until the user types 'exit'.\n",
    "\n",
    "Execution Entry Point : The script uses the standard if `__name__ == \"__main__\":` Python idiom to ensure that `main()` is called when the script is executed directly.\n",
    "\n",
    "For this project, we used the github_repo data loader from LLamaIndex, and you can find its documentation on the Llama Hub. When it comes to loading data from a GitHub repository, the star of the show is the GithubRepositoryReader class. This class is a powerhouse designed to fetch, filter, and format repository data for indexing. Let's break down its key components:\n",
    "\n",
    "1. GitHub Client: You'll first notice that the initialized GitHub client is passed into GithubRepositoryReader. This client provides authenticated access to GitHub repositories.\n",
    "2. Repository Details: Next, the repository owner and name are specified. These are extracted from the URL you input, ensuring the data is fetched from the correct source. This is to give a nice message in the console.\n",
    "3. File Type Filters: One of the most flexible features here is the ability to specify which file types to load. In this example, we're focusing on Python, JavaScript, TypeScript, and Markdown files. The inclusion of Markdown files is so the reader will also pull in README files, offering valuable context for the language model's understanding.\n",
    "4. Verbose Logging: If you're the kind of person who likes to see every detail, you can enable verbose logging. This will print out detailed logs of the data loading process.\n",
    "5. Concurrent Requests: This is where you can speed things up. The number of concurrent requests specifies how many data-fetching operations will happen simultaneously. A word of caution, though: cranking this number up could make you hit GitHub's rate limits, so tread carefully.\n",
    "Once all these parameters are set, the load_data() method swings into action. It fetches the repository data and neatly packages it into a list of Document objects, ready for the next stage— indexing.\n",
    "\n",
    "Indexing: Transforming Data into Queryable Intelligence\n",
    "\n",
    "Vector Store: The first thing that happens is the creation of a DeepLakeVectorStore. Think of this as a specialized database designed to hold vectors. It's a storage unit and an enabler for high-speed queries. The dataset_path parameter specifies where this vector store will reside, and the overwrite flag allows you to control whether existing data should be replaced. The parameter runtime={\"tensor_db\": True} specifies that the vector store will use the Managed Tensor Database for storage and query execution on the Deep Lake infrastructure.\n",
    "The default here is to create a cloud vector DB. Instead, You can create a local vector DB by changing dataset_path to the name of the directory you want to use as DB. For example `dataset_path = \"repository_db\"`\n",
    "The vector store will create a directory with the name specified in dataset_path.\n",
    "\n",
    "Storage Context: Next up is the StorageContext, which essentially acts as a manager for the vector store. It ensures the vector store is accessible and manageable throughout the indexing process.\n",
    "From Documents to Index: The VectorStoreIndex.from_documents() method is the workhorse here. It takes the list of Document objects you got from the GitHub repository and transforms them into a searchable index. This index is stored in the previously initialized DeepLakeVectorStore.\n",
    "The Interactive Finale: Querying and User Engagement\n",
    "After the meticulous data loading and indexing process, the stage is set for the user to interact with the system. This is where the query engine comes into play.\n",
    "\n",
    "Query Engine: Once the index is built, a query engine is initialized using index.as_query_engine(). You'll interact with this engine when you want to search through the GitHub repository. It's optimized for speed and accuracy, ensuring your natural language queries return the most relevant results.\n",
    "Introductory Question: The code starts with an introductory question: \"What is the repository about?\" This serves multiple purposes. It acts as a litmus test to ensure the system is operational and gives the user an immediate sense of what questions can be asked.\n",
    "Formatting and Display: The answer is then formatted to fit within a 100-character width for better readability, thanks to Python's textwrap library.\n",
    "\n",
    "User Input: The code enters an infinite loop, inviting the user to ask questions. The user can type any query related to the GitHub repository, and the system will attempt to provide a relevant answer.\n",
    "\n",
    "Exit Strategy: The loop continues indefinitely until the user types 'exit', providing a simple yet effective way to end the interaction.\n",
    "Query Execution: Each time the user asks a question, the query_engine.query() method is called. This method consults the index built earlier and retrieves the most relevant information.\n",
    "\n",
    "Answer Presentation: Like the introductory question, the answer to the user's query is formatted and displayed. This ensures that regardless of the complexity or length of the solution, it's presented in a readable manner.\n",
    "\n",
    "Now, you're fully equipped to index and query GitHub repositories. To tailor the system to your specific needs, pay attention to the filter_file_extensions parameter. This is where you specify which types of files you'd like to include in your index. The current setting—[\".py\", \".js\", \".ts\", \".md\"]—focuses on Python, JavaScript, TypeScript, and Markdown files.\n",
    "\n",
    "Consider storing these extensions in your .env file for a more dynamic setup. You can easily switch between different configurations without modifying the codebase. It's a best practice that adds a layer of flexibility to your system.\n",
    "\n",
    "Run the command to start:\n",
    "\n",
    "`python3 main.py`\n",
    "\n",
    "The console will ask for a repository URL. To test, I used my repository explaining how to build a ChatGPT plugin using Express.js, so this only has .js extensions.\n",
    "\n",
    "This is how the console will look during the interaction:\n",
    "\n",
    "Copy\n",
    "Please enter the GitHub repository URL: https://github.com/soos3d/chatgpt-plugin-development-quickstart-express\n",
    "Loading chatgpt-plugin-development-quickstart-express repository by soos3d\n",
    "Documents uploaded:\n",
    "{'file_path': 'README.md', 'file_name': 'README.md'}\n",
    "{'file_path': 'index.js', 'file_name': 'index.js'}\n",
    "{'file_path': 'src/app.js', 'file_name': 'app.js'}\n",
    "Uploading to vector store...\n",
    "Your Deep Lake dataset has been successfully created!\n",
    "Dataset(path='hub://YOUR_ORG/repository_db', tensors=['embedding', 'id', 'metadata', 'text'])\n",
    "\n",
    "  tensor      htype      shape     dtype  compression\n",
    "  -------    -------    -------   -------  -------\n",
    " embedding  embedding  (5, 1536)  float32   None\n",
    "    id        text      (5, 1)      str     None\n",
    " metadata     json      (5, 1)      str     None\n",
    "   text       text      (5, 1)      str     None\n",
    "Test question: What is the repository about?\n",
    "==================================================\n",
    "Answer: The repository is a ChatGPT Plugin Quickstart with Express.js. It provides a foundation for\n",
    "developing custom ChatGPT plugins using JavaScript and Express.js. The sample plugin in the\n",
    "repository showcases how ChatGPT can integrate with external APIs, specifically API-Ninja's API, to\n",
    "enhance its capabilities. The plugin fetches airport data based on a city name provided by the user.\n",
    "\n",
    "Please enter your question (or type 'exit' to quit): how does the server work?\n",
    "Your question: how does the server work?\n",
    "==================================================\n",
    "Answer: The server in this context works by setting up an Express.js server with various endpoints to serve\n",
    "a ChatGPT plugin. It initializes the server, configures it to parse JSON in the body of incoming\n",
    "requests, and sets up routes for serving the plugin manifest, OpenAPI schema, logo image, and\n",
    "handling API requests. It also defines a catch-all route to handle any other requests. Finally, the\n",
    "server starts and listens for requests on the specified port.\n",
    "\n",
    "Please enter your question (or type 'exit' to quit): exit\n",
    "Exiting, thanks for chatting!Copy\n",
    "Diving Deeper: LlamaIndex's Low-Level API for customizations\n",
    "While the high-level API of LlamaIndex offers a seamless experience for most use cases, there might be situations where you need more granular control over the query logic. This is where the low-level API shines, offering customizations to fine-tune your interactions with the indexed data.\n",
    "\n",
    "Building the Index\n",
    "To start, you'll need to build an index from your documents, the same as we have done so far.\n",
    "\n",
    "Copy\n",
    "    # Create an index of the documents\n",
    "    try:\n",
    "        vector_store = DeepLakeVectorStore(\n",
    "            dataset_path=dataset_path,\n",
    "            overwrite=True,\n",
    "            runtime={\"tensor_db\": True},\n",
    "    )\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while creating or fetching the vector store: {str(e)}\")\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_documents(docs, storage_context=storage_context)Copy\n",
    "Configuring the Retriever\n",
    "The retriever is responsible for fetching relevant nodes from the index. LlamaIndex supports various retrieval modes, allowing you to choose the one that best fits your needs:\n",
    "\n",
    "Copy\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4)Copy\n",
    "Let's break this down:\n",
    "\n",
    "In modern retrieval systems, documents and queries are represented as vectors, often generated by machine learning models. When a query is made, its vector is compared to document vectors in the index using metrics like cosine similarity. The documents are then ranked based on their similarity scores to the query.\n",
    "\n",
    "Top-k Retrieval: Instead of returning all the documents sorted by their similarity, often only the top k most similar documents are of interest. This is where similarity_top_k comes into play. If similarity_top_k=4, the system will retrieve the top 4 most similar documents to the given query.\n",
    "\n",
    "In the context of the code:\n",
    "\n",
    "Copy\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4)Copy\n",
    "The VectorIndexRetriever is configured to retrieve the top 4 documents most similar to any given query.\n",
    "\n",
    "Benefits of using similarity_top_k:\n",
    "\n",
    "Efficiency: Retrieving only the top-k results can be faster and more memory-efficient than retrieving all results, especially when dealing with large datasets.\n",
    "Relevance: Users are primarily interested in the most relevant results in many applications. By focusing on the top-k results, the system can provide the most pertinent information without overwhelming the user with less relevant results.\n",
    "However, choosing an appropriate value for k is essential. Some relevant results might be missed if k is too small. If k is too large, the system might return more results than necessary, which could be less efficient and potentially less helpful to the user.\n",
    "\n",
    "While creating this guide, I noticed that using a value above '4' for the parameter led the LLM to produce off-context responses.\n",
    "\n",
    "Customize the query engine\n",
    "In LLamaIndex, passing extra parameters and customizations to the query engine is possible.\n",
    "\n",
    "Copy\n",
    "from llama_index import get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever=retriever,\n",
    "        response_mode='default',\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=[\n",
    "            SimilarityPostprocessor(similarity_cutoff=0.7)]\n",
    "    )Copy\n",
    "Let's break down this customization:\n",
    "\n",
    "Getting the Response Synthesizer:\n",
    "\n",
    "Copy\n",
    "response_synthesizer = get_response_synthesizer()Copy\n",
    "Here, the get_response_synthesizer function is called to get an instance of the response synthesizer. The query engine will use this synthesizer to combine and refine the information retrieved by the retriever.\n",
    "\n",
    "Configuring the Query Engine:\n",
    "\n",
    "Copy\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever,\n",
    "    response_mode='default',\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)]\n",
    ")Copy\n",
    "This section configures and initializes the query engine with the following components and settings:\n",
    "\n",
    "retriever: This component fetches relevant nodes (or documents) based on the query. It's passed as an argument, and we set it up in the previous step.\n",
    "response_mode='default': This sets the mode in which the response will be synthesized. The 'default' mode means the system will \"create and refine\" an answer by sequentially going through each retrieved node, making a separate LLM call for each node. This mode is suitable for generating more detailed explanations.\n",
    "response_synthesizer=response_synthesizer: The previously obtained response synthesizer is passed to the query engine. This component will generate the final response using the specified response_mode.\n",
    "node_postprocessors: This is a list of postprocessors that can be applied to the nodes retrieved by the retriever. A SimilarityPostprocessor with a similarity_cutoff of 0.7 is used in this case. This postprocessor filters out nodes based on their similarity scores, ensuring only nodes with a similarity score above 0.7 are considered.\n",
    "All of those are optional except for the retriever; I recommend testing the various modes and values to find what is more suitable for your use case.\n",
    "\n",
    "Exploring Different Response Modes\n",
    "The RetrieverQueryEngine offers various response modes to tailor the synthesis of responses based on the retrieved nodes.\n",
    "\n",
    "Here's a breakdown of some of the available modes:\n",
    "\n",
    "Default Mode:\n",
    "Copy\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='default')Copy\n",
    "In the default mode, the system processes each retrieved node sequentially, making a separate LLM call for each one. This mode is suitable for generating detailed answers.\n",
    "\n",
    "Compact Mode:\n",
    "Copy\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='compact')Copy\n",
    "The compact mode fits as many node text chunks as possible within the maximum prompt size during each LLM call. If there are too many chunks, it refines the answer by processing multiple prompts.\n",
    "\n",
    "Tree Summarize Mode:\n",
    "Copy\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='tree_summarize')Copy\n",
    "This mode constructs a tree from a set of node objects, and the query then returns the root node as the response. It's beneficial for summarization tasks.\n",
    "\n",
    "No Text Mode:\n",
    "Copy\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='no_text')Copy\n",
    "In the no-text mode, the retriever fetches the nodes that would have been sent to the LLM but doesn't send them. This mode allows for inspecting the retrieved nodes without generating a synthesized response.\n",
    "\n",
    "How does LLamaIndex compare to LangChain?\n",
    "Now that you understand how LLamaIndex works comparing it with the big-name LangChain is an excellent time.\n",
    "\n",
    "LlamaIndex and Langchain are both frameworks/libraries designed to enhance the capabilities of large language models by allowing them to interact with external data. While they serve similar overarching goals, their approaches and features differ.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Llama Index:\n",
    "\n",
    "Data Connectors: Offers a variety of connectors to ingest data, making it versatile for different data sources. LLama Hub is an excellent source of community-made tools.\n",
    "Data Structuring: Allows users to structure data using various index types, such as list index, tree index, and even the ability to compose indices.\n",
    "Query Abstraction: Provides layers of abstraction, enabling both simple and complex data querying.\n",
    "Langchain:\n",
    "\n",
    "Modular Design: Comes with modules for tools, agents, and chains, each serving a distinct purpose.\n",
    "Chains: A series of steps or actions the language model takes, ideal for tasks requiring multiple interactions.\n",
    "Agents: Autonomous entities that can decide the next steps in a process, adding a layer of decision-making to the model's interactions.\n",
    "Tools: Utility agents are used to perform specific tasks, such as searching or querying an index.\n",
    "Use Cases:\n",
    "\n",
    "The Llama Index is best suited for applications that require complex data structures and querying. Its strength lies in handling and querying structured data.\n",
    "LangChain, on the other hand, excels in scenarios that require multiple interactions with a language model, especially when those interactions involve decision-making or a series of steps.\n",
    "Of course, you can combine the two; the potential of combining LlamaIndex and Langchain is promising. By integrating LlamaIndex's structured data querying capabilities with the multi-step interaction and decision-making features of Langchain, developers can create robust and versatile applications. As mentioned in the conclusion, This combination can offer the best of both worlds.\n",
    "\n",
    "So which one to use? Use the tool that makes it easier to take care of your use cases; this is usually the leading factor in deciding which one I want to use.\n",
    "\n",
    "Conclusion\n",
    "In this comprehensive guide, we've journeyed through the intricacies of integrating LlamaIndex with Activeloop Deep Lake to create a conversational interface for GitHub repositories. We've seen how these powerful tools can transform a static codebase into an interactive, queryable entity. The synergy between LlamaIndex's data structuring and Deep Lake's optimized storage offers a robust solution for managing and understanding your code repositories.\n",
    "\n",
    "The code we've explored indexes GitHub repositories and makes them accessible through natural language queries. This opens up a plethora of possibilities. Imagine a future where you don't just browse through code; you have conversations with it.\n",
    "\n",
    "This gave you the basics of using LLamaIndex and Deep Lake; try to improve and customize this app to practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
